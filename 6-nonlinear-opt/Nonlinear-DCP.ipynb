{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:27ed86b6429b1263a82aa23b801efdfdbf55ce8097de482f33eda074de9519dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Convex optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we've been thinking about general nonlinear optimization problems of the form\n",
      "\n",
      "\\begin{align}\n",
      "\\min \\quad&f(x)\\\\\n",
      "\\text{s.t.} \\quad& g(x) = 0, \\\\\n",
      "& h(x) \\leq 0.\n",
      "\\end{align}\n",
      "\n",
      "and derivative-based methods to solve them.\n",
      "\n",
      "A special class of nonlinear optimization problems are *convex* optimization problems where $f$ and $h$ are convex and $g$ is affine. Under some additional regularity assumptions, much of the duality theory from linear programming can be extended to convex optimization, and there exist efficient (polynomial-time) algorithms to solve these problems. With few exceptions, if your problem is convex, you can expect to be able to solve it efficiently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Detecting convexity\n",
      "\n",
      "A function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is convex iff $f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y), \\forall x,y \\in \\mathbb{R}^n \\text{ and } \\theta \\in [0,1]$.\n",
      "\n",
      "Given an arbitrary function $f$, detecting if $f$ is convex is [NP-Hard](http://web.mit.edu/~a_a_a/Public/Publications/convexity_nphard.pdf). So how do we know if a problem is convex?\n",
      "\n",
      "A reasonable approach is to make sure that a model is built-up in a manner that lets us prove convexity by using a calculus of convex analysis; this is  **Disciplined Convex Programming** (DCP).\n",
      "\n",
      "We start with operations that are known to be convex:\n",
      "- Norms (why?)\n",
      "- $\\exp(\\cdot)$\n",
      "- $-\\log(\\cdot)$\n",
      "- $x^p$ for $p \\geq 1$ and $x \\geq 0$.\n",
      "- $1/x$ for $x > 0$\n",
      "- $x^2$\n",
      "- ...\n",
      "\n",
      "Then add composition rules, e.g., $f(g(\\cdot))$ is convex when $f$ is convex and\n",
      "- $g$ is linear or affine\n",
      "- $f$ is monotonic increasing and $g$ is convex\n",
      "\n",
      "Also, $f_1+f_2$ and $\\max\\{f_1,f_2\\}$ are convex when $f_1$ and $f_2$ are convex.\n",
      "\n",
      "So our previous example of $x^2 - \\log(x)$ is convex by these rules, because it is the sum of convex functions. So is $\\max\\{e^x,1/x\\}$ ([plot](http://www.wolframalpha.com/input/?i=max%28exp%28x%29%2C1%2Fx%29+for+x+%3E+0)).\n",
      "\n",
      "Note that these rules are *sufficient* but not *necessary* to prove convexity. \n",
      "\n",
      "There are a lot of existing materials on DCP which we won't try to reproduce here. Let's head over to http://dcp.stanford.edu/."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">**\\[Exercise\\]**: DCP Quiz\n",
      "\n",
      "> Play the [DCP quiz](http://dcp.stanford.edu/quiz). Turn up the difficulty to hard for extra fun!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solving \"DCP-compliant\" problems\n",
      "\n",
      "DCP rules are useful not just for proving convexity, but also for *solving* the problems.\n",
      "\n",
      "For example, we (should) know that the following problem\n",
      "\\begin{align}\n",
      "\\min \\quad& {||}x||_1\\\\\n",
      "\\text{s.t.} \\quad& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "where $||x||_1 = \\sum_i |x_i|$ can be solved by using linear programming.\n",
      "\n",
      "Just introduce auxiliary variables $z$ and solve\n",
      "\\begin{align}\n",
      "\\min \\quad& \\sum_i z_i\\\\\n",
      "\\text{s.t.} \\quad&z_i \\geq x_i, \\forall i\\\\\n",
      "& z_i \\geq -x_i, \\forall i\\\\\n",
      "& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "Similarly\n",
      "\\begin{align}\n",
      "\\min \\quad& {||}x||_\\infty\\\\\n",
      "\\text{s.t.} \\quad& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "where $||x||_\\infty = \\max\\{|x_1|,\\cdots,|x_n|\\}$ can be formulated as\n",
      "\n",
      "\\begin{align}\n",
      "\\min \\quad& z\\\\\n",
      "\\text{s.t.} \\quad&z \\geq x_i, \\forall i\\\\\n",
      "& z \\geq -x_i, \\forall i\\\\\n",
      "& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "(What do we do when $||\\cdot||_1$ and $||\\cdot||_\\infty$ appear in convex constraints?)\n",
      "\n",
      "Given these results, we might say that $||\\cdot||_1$ and $||\\cdot||_\\infty$ are *LP-representable*, in a sense that can be made rigorous.\n",
      "\n",
      "What about $||\\cdot||_2$? It's SOCP (second-order conic programming) representable, since\n",
      "$$\n",
      "||x||_2 \\leq t\n",
      "$$\n",
      "is precisely a second-order conic constraint that's already supported by Gurobi, CPLEX, MOSEK, ECOS, SCS, ...\n",
      "\n",
      "What about $1/x$? It's also SOCP representable since\n",
      "$$\n",
      "1/x \\leq t\n",
      "$$\n",
      "iff\n",
      "$$\n",
      "||(2,x-t)||_2 \\leq x+t.\n",
      "$$\n",
      "\n",
      "It turns out that [A LOT](http://docs.mosek.com/generic/modeling-letter.pdf) of common convex functions are SOCP-representable.\n",
      "\n",
      "Once we know how to represent basic operations using LPs or SOCPs, we can easily compose them. For example, we would represent\n",
      "\n",
      "\\begin{align}\n",
      "\\min \\quad& \\max\\{||Cx-d||,1/x_1\\}\\\\\n",
      "\\text{s.t.} \\quad& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "as\n",
      "\n",
      "\\begin{align}\n",
      "\\min \\quad& t\\\\\n",
      "\\text{s.t.} \\quad& t \\geq z_1 \\\\\n",
      "&t \\geq z_2\\\\\n",
      "&{||}Cx-d|| \\leq z_1\\\\\n",
      "&{||}(2,x_1-z_2)|| \\leq x_1+z_2\\\\\n",
      "& Ax = b, \\\\\n",
      "& x \\geq 0,\n",
      "\\end{align}\n",
      "\n",
      "and hand the problem off to Gurobi as an SOCP."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### DCP in summary\n",
      "\n",
      "- Represent the model in a way that makes it easy to use DCP rules to prove convexity.\n",
      "- Break down the individual pieces into parts that are representable using LP, SOCP, semidefinite programming, (or exponential cones)\n",
      "- Use composition rules to *automatically* generate a complete formulation that can be given to existing solvers\n",
      "- Note that derivatives aren't used anywhere!\n",
      "\n",
      "The first implementation of DCP was [CVX](http://cvxr.com/cvx/) in MATLAB. More recently, it's been implemented in [cvxpy](https://github.com/cvxgrp/cvxpy) and [Convex.jl](https://github.com/JuliaOpt/Convex.jl)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Support Vector Machines (SVM)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Support vector machines](http://en.wikipedia.org/wiki/Support_vector_machine) are a popular model in machine learning for classification. We'll use this example to illustrate the basic use of Convex.jl.\n",
      "\n",
      "The basic problem is that we are given a set of N points $x_1,x_2,\\ldots, x_N \\in \\mathbb{R}^n$ and labels $y_1, y_2, \\ldots y_n \\in \\{-1,+1\\}$. And we want to find a hyperplane of the form $w^Tx-b = 0$ that *separates* the two classes, i.e. $w^Tx_i - b \\geq 1$ when $y_i = +1$ and $w^Tx_i - b \\leq -1$ when $y_i = -1$. This condition can be written as $y_i(w^Tx_i - b) \\geq 1, \\forall\\, i$.\n",
      "\n",
      "Such a hyperplane will not exist in general if the data overlap, so instead we'll just try to minimize violations of the constraint $y_i(w^Tx_i - b) \\geq 1, \\forall\\, i$ by adding a penalty when it is violated. The optimization problem can be stated as\n",
      "$$\n",
      "\\min_{w,b} \\sum_{i=1}^N \\left[\\max\\{0, 1 - y_i(w^Tx_i - b)\\}\\right] + \\gamma ||w||_2^2\n",
      "$$\n",
      "Note that we penalize the norm of $w$ in order to guarantee a unique solution.\n",
      "\n",
      "Now let's write our own SVM solver!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "using PyPlot\n",
      "using Convex\n",
      "using ECOS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to generate some random test data\n",
      "function gen_data(N)\n",
      "    # for +1 data, symmetric multivariate normal with center at (1,2)\n",
      "    pos = rand(MvNormal([1.0,2.0],1.0),N)\n",
      "    # for -1 data, symmetric multivariate normal with center at (-1,1)\n",
      "    neg = rand(MvNormal([-1.0,1.0],1.0),N)\n",
      "    x = [pos neg]\n",
      "    y = [fill(+1,N),fill(-1,N)]\n",
      "    return x,y\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see what the data look like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x,y = gen_data(100)\n",
      "plot(x[1,1:100], x[2,1:100], \"ro\", x[1,101:200], x[2,101:200], \"bo\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we translate the optimization problem into Convex.jl form."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "const \u03b3 = 0.005\n",
      "function svm_convex(x,y)\n",
      "    n = size(x,1) # problem dimension\n",
      "    N = size(x,2) # number of points\n",
      "    w = Variable(n)\n",
      "    b = Variable()\n",
      "    \n",
      "    problem = minimize( \u03b3*sum_squares(w) + sum(max(1-y.*(x'*w-b),0)))\n",
      "    solve!(problem, ECOSSolver())\n",
      "    return evaluate(w), evaluate(b)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the solution?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 1000\n",
      "x,y = gen_data(N)\n",
      "\n",
      "plot(x[1,1:N], x[2,1:N], \"ro\", x[1,(N+1):2N], x[2,(N+1):2N], \"bo\");\n",
      "w,b = svm_convex(x,y)\n",
      "\n",
      "@show w,b\n",
      "\n",
      "xmin, xmax = xlim()\n",
      "ymin, ymax = ylim()\n",
      "y1 = (1+b-w[1]*xmin)/w[2]\n",
      "y2 = (1+b-w[1]*xmax)/w[2]\n",
      "plot([xmin,xmax], [y1,y2], \"k-\");\n",
      "y1 = (-1+b-w[1]*xmin)/w[2]\n",
      "y2 = (-1+b-w[1]*xmax)/w[2]\n",
      "plot([xmin,xmax], [y1,y2], \"k-\");\n",
      "y1 = (b-w[1]*xmin)/w[2]\n",
      "y2 = (b-w[1]*xmax)/w[2]\n",
      "ylim(ymin,ymax)\n",
      "plot([xmin,xmax], [y1,y2], \"k-\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">**\\[Exercise\\]**: Sensitivity\n",
      "\n",
      "> Increase the separation between the positive and negative data by modifying the means in ``gen_data``. How does the solution change?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">**\\[Exercise\\]**: JuMP version\n",
      "\n",
      "> Translate the Convex.jl model into a JuMP model with linear constraints and a quadratic objective. For example, ``sum_squares(w)`` becomes ``sum{w[i]^2,i=1:n}``. Hint: the formulation is given on Wikpedia. (You may want to use ``IpoptSolver`` since ``ECOSSolver`` supports second-order conic constraints but won't directly accept quadratic objectives.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Discussion\n",
      "\n",
      "- Convex.jl vs. JuMP\n",
      "- Derivative-based nonlinear vs. automatic transformation to LP/SOCP/conic form"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}